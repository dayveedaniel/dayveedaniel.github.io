<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Dimensionality Reduction and Feature Selection Algorithms</title>

    <!-- Include Reveal.js CSS -->
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/reveal.js@4.3.1/dist/reveal.css">
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/reveal.js/5.1.0/theme/solarized.min.css">

    <!-- Include MathJax for rendering mathematical formulas -->
    <script src="https://cdn.jsdelivr.net/npm/mathjax@2.7.7/MathJax.js?config=TeX-MML-AM_CHTML"></script>

    <style>
        /* Custom CSS for code block styling */
        pre code {
            background-color: #f4f4f4;
            padding: 10px;
            border-radius: 5px;
            border: 1px solid #ddd;
            font-size: 0.9em;
            white-space: pre-wrap;
            word-wrap: break-word;
        }

        /* Styling for scrollable code blocks */
        .code-block {
            max-height: 300px; /* Ограничиваем высоту блока кода */
            overflow-y: scroll;  /* Добавляем прокрутку */
            margin-bottom: 20px;
        }

        /* Скрываем видимую прокрутку, но оставляем возможность прокрутки */
        .code-block::-webkit-scrollbar {
            width: 0px;  /* Скрыть вертикальную прокрутку */
            background: transparent;  /* Делает фон прокрутки прозрачным */
        }

        .code-block::-webkit-scrollbar-thumb {
            background: transparent;  /* Скрыть ползунок прокрутки */
        }

        /* Ensure slides are scrollable and content fits nicely */
        .reveal section {
            padding: 10px 20px;
            font-size: 1em; /* Reduced font size */
            line-height: 1.4;
            max-height: 85vh; /* Set a max-height for better scrolling */
            overflow-y: auto; /* Allow vertical scrolling if content exceeds max-height */
            box-sizing: border-box;
        }

        /* Adjust the layout for math formulas */
        .math {
            font-size: 1.2em;
            color: #2a2a2a;
        }

        ul {
            font-size: 1em;
            margin-top: 10px;
        }

        li {
            margin-bottom: 10px;
        }

        h2, h3 {
            font-size: 1.3em;
            margin-bottom: 15px;
        }

        /* Custom styling for slide content and text */
        .content-wrapper {
            max-height: 75vh;
            overflow: auto;
            padding: 10px;
        }

        .slide-heading {
            font-size: 1.5em;
            margin-bottom: 10px;
        }

        .custom-section {
            overflow-y: scroll !important;
        }

        /* Hide scrollbar for Chrome, Safari and Opera */
        .hide-scrollbar::-webkit-scrollbar {
            display: none !important;
        }

        /* Hide scrollbar for IE, Edge and Firefox */
        .hide-scrollbar {
            -ms-overflow-style: none !important;  /* IE and Edge */
            scrollbar-width: none !important;  /* Firefox */
        }

        .show-overflow {
            overflow-y: visible;   
            max-height: 100% !important;         
        }
    </style>
</head>
<body>

    <!-- Presentation Container -->
    <div class="reveal">
        <div class="slides">
            <!-- Title Slide -->
            <section class="custom-section hide-scrollbar show-overflow">
                <h1 style="font-size: 3em; margin-top: 30px;">ASE PROJECT TEAM03</h1>
                <h2 style="font-size: 2em;">Marina Kotelevskaya, Daniel David, Nikita Yaneev</h2>
                <h3 style="font-size: 1.5em; margin-top: 20px;">September, 2024</h3>
            </section>

            <!-- Table of Contents Slide -->
            <section class="custom-section hide-scrollbar show-overflow">
                <h2>Outline</h2>
                <ul>
                    <li>Introduction</li>
                    <li>Problem Statement</li>
                    <li>Algorithm Overview</li>
                    <li>Methodology</li>
                    <li>Results</li>
                    <li>Discussion</li>
                    <li>Conclusion</li>
                    <li>References</li>
                </ul>
            </section>

            <!-- Introduction Slide -->
            <section class="custom-section hide-scrollbar show-overflow">
                <h2 class="slide-heading">Introduction</h2>
                <div class="content-wrapper hide-scrollbar show-overflow">
                    <ul>
                        <li>Feature selection simplifies models and reduces complexity.</li>
                        <li>Dimensionality reduction algorithms help by selecting the most informative features.</li>
                        <li>We explore two optimization algorithms: <strong>Equilibrium Optimizer (EO)</strong> and <strong>Differential Evolution (DE)</strong> for feature selection.</li>
                    </ul>
                </div>
            </section>

            <!-- Python Libraries Slide -->
            <section class="custom-section hide-scrollbar show-overflow">
                <h2 class="slide-heading">Python Libraries</h2>
                <div class="content-wrapper hide-scrollbar show-overflow">
                    <h3><strong>numpy</strong></h3>
                    <ul>
                        <li><strong>Purpose:</strong> Efficient numerical computations for handling large arrays and matrices.</li>
                        <li><strong>Usage in our algorithm:</strong> 
                            <ul>
                                <li><strong>Population Initialization:</strong> The initial population of feature subsets is generated randomly:</li>
                            </ul>
                            <pre><code>
def initialize_population(self):
    population = np.zeros((self.part_count, self.dimension))
    for i in range(self.part_count):
        num_features = np.random.randint(self.min_features, self.max_features + 1)
        selected_features = np.random.choice(self.dimension, num_features, replace=False)
        population[i, selected_features] = 1
    return population
                            </code></pre>
                        <li><strong>Probability-Based Updates:</strong> Feature selection is updated using a sigmoid-based probability:</li>
                        <pre><code>
prob = 1 / (1 + np.exp(-new_position))
new_position = (prob > 0.5).astype(int)
                        </code></pre>
                    </ul>

                    <h3><strong>pandas</strong></h3>
                    <ul>
                        <li><strong>Purpose:</strong> Data manipulation and preprocessing.</li>
                        <li><strong>Usage in our algorithm:</strong>
                            <ul>
                                <li><strong>Reading and Scaling Data:</strong> The input dataset is read and standardized:</li>
                            </ul>
                            <pre><code>
df = pd.read_csv("data/data_all.csv", index_col=0)
scaled_data = self.scaler.fit_transform(df)
                            </code></pre>
                            <li><strong>Scaling formula:</strong>
                                \[
                                X_{\text{scaled}} = \frac{X - \mu}{\sigma}
                                \]
                                where \( X \) is the original data matrix, \( \mu \) is the mean of each feature, and \( \sigma \) is the standard deviation.
                            </li>
                        </li>
                    </ul>

                    <h3><strong>scikit-learn</strong></h3>
                    <ul>
                        <li><strong>Purpose:</strong> Preprocessing and metric calculations.</li>
                        <li><strong>Usage in our algorithm:</strong>
                            <ul>
                                <li><strong>Distance Calculation:</strong> Pairwise distances are calculated using the Euclidean distance formula:</li>
                            </ul>
                            <pre><code>
from sklearn.metrics.pairwise import euclidean_distances
                            </code></pre>
                        </li>
                    </ul>

                    <h3><strong>matplotlib</strong></h3>
                    <ul>
                        <li><strong>Purpose:</strong> Visualization of results.</li>
                        <li><strong>Usage in our algorithm:</strong> Plots are generated to visualize the convergence of fitness over iterations and feature importance scores.</li>
                    </ul>
                </div>
            </section>

            <!-- Custom Methods and Functions Slide -->
            <section class="custom-section hide-scrollbar show-overflow">
                <h2 class="slide-heading">Custom Methods and Functions</h2>
                <div class="content-wrapper hide-scrollbar show-overflow">
                    <h3>Population Initialization</h3>
                    <pre><code>
def initialize_population(self):
    population = np.zeros((self.part_count, self.dimension))
    for i in range(self.part_count):
        num_features = np.random.randint(self.min_features, self.max_features + 1)
        selected_features = np.random.choice(self.dimension, num_features, replace=False)
        population[i, selected_features] = 1
    return population
                    </code></pre>
                    <h3>Equilibrium Pool Update</h3>
                    <pre><code>
def initialize_population(self):
    current_fitness = np.array([self.calculate_fitness(p, data) for p in population])
    sorted_indices = np.argsort(current_fitness)
    eqPool = population[sorted_indices[:self.pool_size]]
                    </code></pre>

                    <h3>Position Updates</h3>
                    <pre><code>
def initialize_population(self):
    F = self.a1 * np.sign(r1 - 0.5) * (1 - np.exp(-2 * t))
    if r2 < 0.5:
        new_position = eq_candidate + F * (r3 * (eqPool[0] - population[i]))
    else:
        lambda_val = (1 - r2) * r3
        new_position = eq_candidate + F * (lambda_val * (eq_candidate - population[i]))
                    </code></pre>
                    <p class="math">
                        \( x_{\text{new}} = x_{\text{eq}} + F \cdot \left( \lambda \cdot (x_{\text{eq}} - x) \right) \)
                    </p>
                </div>
            </section>
            <!-- Equilibrium Optimizer (EO) Slide -->
            <section class="custom-section hide-scrollbar show-overflow">
                <h2>Equilibrium Optimizer (EO)</h2>
                <p>The Equilibrium Optimizer (EO) is a heuristic optimization algorithm that balances exploration and exploitation by updating candidate solutions (particles) towards equilibrium candidates.</p>
            
                <h3>1. Initialization:</h3>
                <p>The population of particles is initialized randomly, with each particle representing a subset of features:</p>
                <p class="math">
                    \( \mathbf{P}_i = \{p_{ij} \mid p_{ij} \in \{0, 1\}\}, \quad j = 1, 2, \dots, d \)
                </p>
            
                <h3>2. Equilibrium Pool Update:</h3>
                <p>The equilibrium pool stores the best solutions based on their fitness:</p>
                <p class="math">
                    \( \mathbf{F}_{\text{best}} = \arg\min_{k} \text{Fitness}(\mathbf{P}_k) \)
                </p>
            
                <h3>3. Position Update:</h3>
                <p>The particle updates its position towards an equilibrium candidate using:</p>
                <p class="math">
                    \( x_{\text{new}} = x_{\text{eq}} + F \cdot \left( \lambda \cdot (x_{\text{eq}} - x) \right) \)
                </p>
                <ul>
                    <li>\( x_{\text{new}} \): new position of the particle.</li>
                    <li>\( x_{\text{eq}} \): equilibrium candidate solution.</li>
                    <li>\( F \): control factor modulating the step size.</li>
                    <li>\( \lambda \): random weight determining the move direction.</li>
                </ul>
            </section>

            <!-- Fitness Functions Slide -->
            <section class="custom-section hide-scrollbar show-overflow">
                <h2 class="slide-heading">Fitness Functions</h2>
                <div class="content-wrapper hide-scrollbar show-overflow">
                    <h3>Sammon Error</h3>
                    <p class="math">
                        \( E_{\text{Sammon}} = \sum_{i \neq j} \frac{\left( d_{ij} - \hat{d}_{ij} \right)^2}{d_{ij}} \)
                    </p>
                    <!-- Add code block with scrollable container -->
                    <div class="code-block">
                        <pre><code>
            def calculate_sammon_error(original_data, reduced_data):
                return np.sum(((original_data - reduced_data)**2) / original_data)
                        </code></pre>
                    </div>
            
                    <h3>Kruskal Stress</h3>
                    <p class="math">
                        \( E_{\text{Kruskal}} = \sqrt{\frac{\sum_{i \neq j} \left( d_{ij} - \hat{d}_{ij} \right)^2}{\sum_{i \neq j} d_{ij}^2}} \)
                    </p>
                    <!-- Add code block with scrollable container -->
                    <div class="code-block">
                        <pre><code>
            def calculate_kruskal_stress(original_data, reduced_data):
                return np.sqrt(np.sum((original_data - reduced_data)**2) / np.sum(original_data**2))
                        </code></pre>
                    </div>
                </div>
            </section>            

            <!-- Hypotheses and Validation Slide -->
            <section class="custom-section hide-scrollbar show-overflow">
                <h2>Hypotheses and Validation</h2>
            
                <h3>1. Internal Validity</h3>
                <p>The internal validity of EO was assessed by comparing its performance across different datasets using two common error metrics: Sammon Error and Kruskal Stress.</p>
            
                <h4>Hypothesis Testing:</h4>
                <p class="math" style="font-size: 1.2em;">
                    \( H_0: F_{\text{EO}}(x) = \text{Constant}, \quad \text{vs.} \quad H_a: F_{\text{EO}}(x) \neq \text{Constant} \)
                </p>

                <h4>Mann-Whitney U Test:</h4>
                <p class="math" style="font-size: 1.2em;">
                    \( U = n_1 \cdot n_2 + \frac{n_1(n_1+1)}{2} - R_1 \)
                </p>
                <ul>
                    <li>\( n_1, n_2 \): sample sizes for each strategy</li>
                    <li>\( R_1 \): sum of ranks for EO-selected features</li>
                </ul>
            
                <h3>2. External Validity</h3>
                <p>The generalizability of EO was validated on 32 repositories distinct from the training set.</p>
            
                <h4>Mean Error Analysis:</h4>
                <p class="math" style="font-size: 1.2em;">
                    \( H_0: \mu_{\text{val}} \geq \epsilon, \quad H_a: \mu_{\text{val}} < \epsilon \)
                </p>
                <ul>
                    <li>\( \mu_{\text{val}} \): sample mean error of validation set</li>
                </ul>
            
                <h3>3. Convergent Validity</h3>
                <p>The similarity between EO-selected features and alternative methods was evaluated using the Jaccard Similarity Index:</p>
                <p class="math" style="font-size: 1.2em;">
                    \( J(A, B) = \frac{|A \cap B|}{|A \cup B|} \)
                </p>
                <ul>
                    <li>\( A \) and \( B \): feature subsets selected by EO and another method</li>
                </ul>
            
                <h3>Summary of Validity Metrics</h3>
                <table>
                    <tr>
                        <th>Validity Aspect</th>
                        <th>Test/Metric</th>
                        <th>Result</th>
                    </tr>
                    <tr>
                        <td>Internal Validity</td>
                        <td>Mann-Whitney U Test</td>
                        <td>\( p < 0.05 \)</td>
                    </tr>
                    <tr>
                        <td>Internal Validity</td>
                        <td>Standard Deviation (\( \sigma \))</td>
                        <td>\(Lower for EO\)</td>
                    </tr>
                    <tr>
                        <td>External Validity</td>
                        <td>One-Sample \( t \)-Test</td>
                        <td>\( p < 0.01 \)</td>
                    </tr>
                    <tr>
                        <td>External Validity</td>
                        <td>Mean Error Threshold (\( \epsilon = 0.25 \))</td>
                        <td>\(Met\)</td>
                    </tr>
                    <tr>
                        <td>Convergent Validity</td>
                        <td>Jaccard Similarity Index</td>
                        <td>\( J > 0.8 \)</td>
                    </tr>
                </table>
            
                <p>This analysis confirms that EO is both robust and generalizable, preserving the geometric relationships in datasets while selecting minimal subsets of metrics.</p>
            </section>

            <!-- Methodology for Differential Evolution (DE) Slide -->
            <section class="custom-section hide-scrollbar show-overflow">
                <h2>Methodology for Differential Evolution (DE)</h2>
            
                <p><strong>Differential Evolution (DE)</strong> is an optimization algorithm using a population-based approach with real-valued parameters. It excels in solving nonlinear and non-smooth optimization problems.</p>
            
                <h3>Key Features:</h3>
                <ul>
                    <li><strong>Real-valued parameters:</strong> DE works with real numbers, unlike binary genetic algorithms.</li>
                    <li><strong>Mutation:</strong> A difference between two randomly selected population vectors is added to a third, enabling precise adjustments.</li>
                    <li><strong>Faster Convergence:</strong> DE converges quickly to optimal solutions, especially in continuous spaces.</li>
                </ul>
            
                <h3>Mutation Formula:</h3>
                <p class="math" style="font-size: 1.2em; line-height: 1.5;">
                    \( \mathbf{mutant} = \mathbf{r_1} + F \cdot (\mathbf{r_2} - \mathbf{r_3}) \)
                </p>
            
                <h3>Gray Code Application in DE:</h3>
                <ul>
                    <li><strong>Reduces quantization errors:</strong> Useful for discrete optimization tasks.</li>
                    <li><strong>Stabilizes mutation:</strong> Minimizes large jumps during optimization.</li>
                </ul>
            </section>
            <!-- Optimization Process in DE Slide -->
            <section class="custom-section hide-scrollbar show-overflow">
                <h2>Optimization Process in DE</h2>
            
                <p>The DE algorithm involves four key operations: initialization, mutation, crossover, and selection.</p>
            
                <h3>1. Initialization of the Population:</h3>
                <p>A random population \( P = \{x_1, x_2, \dots, x_N\} \) is initialized:</p>
                <p class="math" style="font-size: 1.2em; line-height: 1.5;">
                    \( x_{i,j}^0 = x_{\text{min},j} + r_j \cdot (x_{\text{max},j} - x_{\text{min},j}), \quad i = 1, 2, \dots, N \)
                </p>
                <ul>
                    <li> \( r_j \sim U(0, 1) \): Random number generator</li>
                    <li> \( x_{\text{min},j}, x_{\text{max},j} \): Lower and upper bounds for feature \( j \)</li>
                </ul>
            
                <h3>2. Mutation:</h3>
                <p>For each target vector \( x_i \), a mutant vector \( v_i \) is generated:</p>
                <p class="math" style="font-size: 1.2em; line-height: 1.5;">
                    \( v_i = x_{r1} + F \cdot (x_{r2} - x_{r3}), \quad r1 \neq r2 \neq r3 \neq i \)
                </p>
                <ul>
                    <li> \( F \): Scaling factor (controls the mutation's strength)</li>
                    <li> \( x_{r1}, x_{r2}, x_{r3} \): Distinct randomly chosen population vectors</li>
                </ul>
            
                <h3>3. Crossover:</h3>
                <p>A trial vector \( u_i \) is created using the crossover operator:</p>
                <p class="math" style="font-size: 1.2em; line-height: 1.5;">
                    \( u_{i,j} = \begin{cases} 
                        v_{i,j}, & \text{if } r_j \leq CR \text{ or } j = j_{\text{rand}}, \\
                        x_{i,j}, & \text{otherwise}.
                    \end{cases} \)
                </p>
                <ul>
                    <li> \( CR \): Crossover rate (determines the probability of replacing the target vector's feature)</li>
                    <li> \( j_{\text{rand}} \): Random index for ensuring diversity in the crossover</li>
                </ul>
            
                <h3>4. Selection:</h3>
                <p>The trial vector replaces the target vector if it has a better fitness:</p>
                <p class="math" style="font-size: 1.2em; line-height: 1.5;">
                    \( x_i^{t+1} = \begin{cases} 
                        u_i, & \text{if } f(u_i) < f(x_i), \\
                        x_i, & \text{otherwise}.
                    \end{cases} \)
                </p>
                <ul>
                    <li> \( f(u_i) \): Fitness of the trial vector</li>
                    <li> \( f(x_i) \): Fitness of the target vector</li>
                </ul>
            </section>

            <!-- Results Slide -->
            <section class="custom-section hide-scrollbar show-overflow">
                <h2 class="slide-heading">Results</h2>
                <div class="content-wrapper hide-scrollbar show-overflow">
                    <ul>
                        <li>DE and EO reduced the dataset from 30 features to 5, preserving over 95% of the original structure.</li>
                        <li>The selected features maintained key relationships between the data points.</li>
                        <li>Sammon Error and Kruskal Stress showed low values, indicating high preservation of original structure.</li>
                    </ul>
                </div>
            </section>

            <!-- Conclusion -->
            <section class="custom-section hide-scrollbar show-overflow">
                <h2 class="slide-heading">Conclusion</h2>
                <div class="content-wrapper hide-scrollbar show-overflow">
                    <ul>
                        <li>DE and EO are powerful tools for feature selection and dimensionality reduction.</li>
                        <li>Both algorithms are effective in reducing feature sets while preserving the structure of the data.</li>
                        <li>Future work could explore hybrid approaches and scalability to larger datasets.</li>
                    </ul>
                </div>
            </section>

            <!-- References Slide -->
            <section class="custom-section hide-scrollbar show-overflow">
                <h2>References</h2>
            
                <ol>
                    <li>
                        <strong>R. Storn and K. Price,</strong><br>
                        <em>Differential Evolution – A Simple and Efficient Heuristic for Global Optimization over Continuous Spaces,</em><br>
                        Journal of Global Optimization, vol. 11, no. 4, pp. 341–359, 1997.<br>
                        <a href="https://doi.org/10.1023/A:1008202821328" target="_blank">https://doi.org/10.1023/A:1008202821328</a>
                    </li>
                
                    <li>
                        <strong>J. W. Sammon,</strong><br>
                        <em>A Nonlinear Mapping for Data Structure Analysis,</em><br>
                        IEEE Transactions on Computers, vol. C-18, no. 5, pp. 401–409, 1969.<br>
                        <a href="https://doi.org/10.1109/T-C.1969.222678" target="_blank">https://doi.org/10.1109/T-C.1969.222678</a>
                    </li>
                
                    <li>
                        <strong>S. Faramarzi, M. Heidarinejad, B. Stephens, and S. Mirjalili,</strong><br>
                        <em>Equilibrium Optimizer: A Novel Optimization Algorithm,</em><br>
                        Knowledge-Based Systems, vol. 191, 105190, 2020.<br>
                        <a href="https://doi.org/10.1016/j.knosys.2019.105190" target="_blank">https://doi.org/10.1016/j.knosys.2019.105190</a>
                    </li>
                
                    <li>
                        <strong>J. B. Kruskal,</strong><br>
                        <em>Multidimensional Scaling by Optimizing Goodness of Fit to a Nonmetric Hypothesis,</em><br>
                        Psychometrika, vol. 29, no. 1, pp. 1–27, 1964.<br>
                        <a href="https://doi.org/10.1007/BF02289565" target="_blank">https://doi.org/10.1007/BF02289565</a>
                    </li>
                </ol>
            </section>


        </div>
    </div>

    <script src="https://cdn.jsdelivr.net/npm/reveal.js@4.3.1/dist/reveal.js"></script>
    <script>
        // Initialize Reveal.js
        Reveal.initialize({
            controls: true,
            progress: true,
            history: true,
            center: true,
            transition: 'slide', // Can also be 'fade', 'convex', 'zoom'
            slideNumber: true,   // Show slide number
            transitionSpeed: 'fast', // Speed of transitions
            fragments: true      // Enable fragments for interactive effects
        });
    </script>

</body>
</html>
